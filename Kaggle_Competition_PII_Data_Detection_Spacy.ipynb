{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook is by Anastasia Ruzmaikina for Kaggle Competition Learning Agency Lab - PII Data Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this competition is to develop a model that detects personally identifiable information (PII) in student writing. Your efforts to automate the detection and removal of PII from educational data will lower the cost of releasing educational datasets. This will support learning science research and the development of educational tools.\n",
    "\n",
    "Reliable automated techniques could allow researchers and industry to tap into the potential that large public educational datasets offer to support the development of effective tools and interventions for supporting teachers and students.\n",
    "\n",
    "In today’s era of abundant educational data from sources such as ed tech, online learning, and research, widespread PII is a key challenge. PII’s presence is a barrier to analyze and create open datasets that advance education because releasing the data publicly puts students at risk. To reduce these risks, it’s crucial to screen and cleanse educational data for PII before public release, which data science could streamline.\n",
    "Manually reviewing the entire dataset for PII is currently the most reliable screening method, but this results in significant costs and restricts the scalability of educational datasets. While techniques for automatic PII detection that rely on named entity recognition (NER) exist, these work best for PII that share common formatting such as emails and phone numbers. PII detection systems struggle to correctly label names and distinguish between names that are sensitive (e.g., a student's name) and those that are not (e.g., a cited author)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses Spacy Presidio-Analyzer to detect PII in student essays. Llama-2-7b-chat was too slow given the size of the dataset. The accuracy score of this notebook on the competition test dataset is 68%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-20T02:36:56.868127Z",
     "iopub.status.busy": "2024-03-20T02:36:56.867146Z",
     "iopub.status.idle": "2024-03-20T02:36:56.899524Z",
     "shell.execute_reply": "2024-03-20T02:36:56.898639Z",
     "shell.execute_reply.started": "2024-03-20T02:36:56.868091Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:36:56.902106Z",
     "iopub.status.busy": "2024-03-20T02:36:56.901659Z",
     "iopub.status.idle": "2024-03-20T02:36:56.906396Z",
     "shell.execute_reply": "2024-03-20T02:36:56.905529Z",
     "shell.execute_reply.started": "2024-03-20T02:36:56.902074Z"
    }
   },
   "outputs": [],
   "source": [
    "#!sudo apt install python-dev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:36:56.907755Z",
     "iopub.status.busy": "2024-03-20T02:36:56.907460Z",
     "iopub.status.idle": "2024-03-20T02:40:15.925632Z",
     "shell.execute_reply": "2024-03-20T02:40:15.924390Z",
     "shell.execute_reply.started": "2024-03-20T02:36:56.907730Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install -qq --no-deps /kaggle/input/boto3-13461/boto3-1.34.61-py3-none-any.whl\n",
    "#!pip install -qq --no-deps /kaggle/input/es-core-news-sm/es_core_news_sm-2.3.1\n",
    "!pip install -qq --no-deps /kaggle/input/es-core-news/es_core_news_md-3.7.0-py3-none-any.whl\n",
    "!pip install -qq --no-deps /kaggle/input/phonenumbers/phonenumbers-8.13.32-py2.py3-none-any.whl\n",
    "!pip install -qq --no-deps /kaggle/input/presidio-analyzer-and-anonymizer-v2-2-351/presidio_analyzer-2.2.351-py3-none-any.whl\n",
    "!pip install -qq --no-deps /kaggle/input/presidio-analyzer-and-anonymizer-v2-2-351/presidio_anonymizer-2.2.351-py3-none-any.whl\n",
    "!pip install -qq --no-deps /kaggle/input/presidio-analyzer/presidio_analyzer-2.2.353-py3-none-any.whl\n",
    "!pip install -qq --no-deps /kaggle/input/requests-file/requests_file-2.0.0-py2.py3-none-any.whl\n",
    "!pip install -qq --no-deps /kaggle/input/sagemaker/sagemaker-2.212.0-py3-none-any.whl\n",
    "#!pip install -qq --no-deps /kaggle/input/script-deb-sh/script.deb.sh\n",
    "!pip install -qq --no-deps /kaggle/input/tldextract/tldextract-5.1.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:15.927666Z",
     "iopub.status.busy": "2024-03-20T02:40:15.927288Z",
     "iopub.status.idle": "2024-03-20T02:40:15.932264Z",
     "shell.execute_reply": "2024-03-20T02:40:15.931352Z",
     "shell.execute_reply.started": "2024-03-20T02:40:15.927630Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -qq --no-deps /kaggle/input/es-core-news-sm/es_core_news_sm-2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:15.936647Z",
     "iopub.status.busy": "2024-03-20T02:40:15.935856Z",
     "iopub.status.idle": "2024-03-20T02:40:17.629117Z",
     "shell.execute_reply": "2024-03-20T02:40:17.628050Z",
     "shell.execute_reply.started": "2024-03-20T02:40:15.936611Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_json('/kaggle/input/pii-detection-removal-from-educational-data/train.json')\n",
    "df_test = pd.read_json('/kaggle/input/pii-detection-removal-from-educational-data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:17.631178Z",
     "iopub.status.busy": "2024-03-20T02:40:17.630728Z",
     "iopub.status.idle": "2024-03-20T02:40:17.635950Z",
     "shell.execute_reply": "2024-03-20T02:40:17.634969Z",
     "shell.execute_reply.started": "2024-03-20T02:40:17.631140Z"
    }
   },
   "outputs": [],
   "source": [
    "#not commented out when internet on\n",
    "#!pip install sagemaker --upgrade\n",
    "#import sagemaker\n",
    "#!pip install /kaggle/input/d/hashidoyuto/presidio-analyzer/presidio_analyzer\n",
    "#assert sagemaker.__version__ >= \"2.75.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:17.637699Z",
     "iopub.status.busy": "2024-03-20T02:40:17.637393Z",
     "iopub.status.idle": "2024-03-20T02:40:17.646413Z",
     "shell.execute_reply": "2024-03-20T02:40:17.645377Z",
     "shell.execute_reply.started": "2024-03-20T02:40:17.637675Z"
    }
   },
   "outputs": [],
   "source": [
    "#not commented out when internet on\n",
    "#!sudo apt-get update -y\n",
    "#!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "#!sudo apt-get install git-lfs git -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:17.648022Z",
     "iopub.status.busy": "2024-03-20T02:40:17.647672Z",
     "iopub.status.idle": "2024-03-20T02:40:17.656646Z",
     "shell.execute_reply": "2024-03-20T02:40:17.655753Z",
     "shell.execute_reply.started": "2024-03-20T02:40:17.647972Z"
    }
   },
   "outputs": [],
   "source": [
    "#import sagemaker\n",
    "#import boto3\n",
    "#sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "#sagemaker_session_bucket=None\n",
    "#if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "   # sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "#try:\n",
    "    #role = sagemaker.get_execution_role()\n",
    "#except ValueError:\n",
    "    #iam = boto3.client('iam')\n",
    "    #role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "#sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "#print(f\"sagemaker role arn: {role}\")\n",
    "#print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "#print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:17.658096Z",
     "iopub.status.busy": "2024-03-20T02:40:17.657775Z",
     "iopub.status.idle": "2024-03-20T02:40:17.669722Z",
     "shell.execute_reply": "2024-03-20T02:40:17.668742Z",
     "shell.execute_reply.started": "2024-03-20T02:40:17.658068Z"
    }
   },
   "outputs": [],
   "source": [
    "#not commented out when internet on\n",
    "#!pip install presidio_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:17.671303Z",
     "iopub.status.busy": "2024-03-20T02:40:17.670921Z",
     "iopub.status.idle": "2024-03-20T02:40:17.679256Z",
     "shell.execute_reply": "2024-03-20T02:40:17.678288Z",
     "shell.execute_reply.started": "2024-03-20T02:40:17.671269Z"
    }
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import PatternRecognizer\n",
    "titles_recognizer = PatternRecognizer(supported_entity=\"TITLE\",\n",
    "                                      deny_list=[\"Mr.\",\"Mrs.\",\"Miss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:17.680677Z",
     "iopub.status.busy": "2024-03-20T02:40:17.680434Z",
     "iopub.status.idle": "2024-03-20T02:40:17.691825Z",
     "shell.execute_reply": "2024-03-20T02:40:17.690948Z",
     "shell.execute_reply.started": "2024-03-20T02:40:17.680658Z"
    }
   },
   "outputs": [],
   "source": [
    "titles_recognizer.analyze(text=\"Mr. Schmidt\", entities=\"TITLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:17.693387Z",
     "iopub.status.busy": "2024-03-20T02:40:17.692853Z",
     "iopub.status.idle": "2024-03-20T02:40:20.284017Z",
     "shell.execute_reply": "2024-03-20T02:40:20.283127Z",
     "shell.execute_reply.started": "2024-03-20T02:40:17.693357Z"
    }
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "\n",
    "registry = RecognizerRegistry()\n",
    "registry.load_predefined_recognizers()\n",
    "\n",
    "# Add the recognizer to the existing list of recognizers\n",
    "registry.add_recognizer(titles_recognizer)\n",
    "\n",
    "# Set up analyzer with our updated recognizer registry\n",
    "analyzer = AnalyzerEngine(registry=registry)\n",
    "\n",
    "# Run with input text\n",
    "text=\"His name is Mr. Jones\"\n",
    "results = analyzer.analyze(text=text, language=\"en\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:20.285423Z",
     "iopub.status.busy": "2024-03-20T02:40:20.285154Z",
     "iopub.status.idle": "2024-03-20T02:40:22.851846Z",
     "shell.execute_reply": "2024-03-20T02:40:22.850960Z",
     "shell.execute_reply.started": "2024-03-20T02:40:20.285399Z"
    }
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine\n",
    "\n",
    "analyzer = AnalyzerEngine()\n",
    "\n",
    "analyzer.registry.add_recognizer(titles_recognizer)\n",
    "\n",
    "results = analyzer.analyze(text=text, language=\"en\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:22.857365Z",
     "iopub.status.busy": "2024-03-20T02:40:22.857038Z",
     "iopub.status.idle": "2024-03-20T02:40:25.512935Z",
     "shell.execute_reply": "2024-03-20T02:40:25.511993Z",
     "shell.execute_reply.started": "2024-03-20T02:40:22.857338Z"
    }
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "\n",
    "import regex as re\n",
    "\n",
    "registry = RecognizerRegistry(global_regex_flags=re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
    "engine = AnalyzerEngine(registry=registry)\n",
    "engine.analyze(text, language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:25.514370Z",
     "iopub.status.busy": "2024-03-20T02:40:25.514088Z",
     "iopub.status.idle": "2024-03-20T02:40:30.718691Z",
     "shell.execute_reply": "2024-03-20T02:40:30.717776Z",
     "shell.execute_reply.started": "2024-03-20T02:40:25.514345Z"
    }
   },
   "outputs": [],
   "source": [
    "#this works!!!\n",
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "# Create configuration containing engine name and models\n",
    "configuration = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [{\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n",
    "                {\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n",
    "}\n",
    "\n",
    "# Create NLP engine based on configuration\n",
    "provider = NlpEngineProvider(nlp_configuration=configuration)\n",
    "nlp_engine_with_spanish = provider.create_engine()\n",
    "\n",
    "# Pass the created NLP engine and supported_languages to the AnalyzerEngine\n",
    "analyzer = AnalyzerEngine(\n",
    "    nlp_engine=nlp_engine_with_spanish, \n",
    "    supported_languages=[\"en\", \"es\"]\n",
    ")\n",
    "\n",
    "# Analyze in different languages\n",
    "results_spanish = analyzer.analyze(text=\"Mi nombre es Morris\", language=\"es\")\n",
    "print(results_spanish)\n",
    "\n",
    "results_english = analyzer.analyze(text=\"My name is Morris\", language=\"en\")\n",
    "print(results_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:30.720409Z",
     "iopub.status.busy": "2024-03-20T02:40:30.720046Z",
     "iopub.status.idle": "2024-03-20T02:40:30.730355Z",
     "shell.execute_reply": "2024-03-20T02:40:30.729457Z",
     "shell.execute_reply.started": "2024-03-20T02:40:30.720376Z"
    }
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import EntityRecognizer, RecognizerResult\n",
    "class TransformersRecognizer(EntityRecognizer):\n",
    "    def __init__(self,model_id_or_path=None,aggregation_strategy=\"average\",supported_language=\"en\",ignore_labels=[\"O\",\"MISC\"]):\n",
    "      # inits transformers pipeline for given mode or path\n",
    "      self.pipeline = pipeline(\"token-classification\",model=model_id_or_path,aggregation_strategy=\"average\",ignore_labels=ignore_labels)\n",
    "      # map labels to presidio labels\n",
    "      self.label2presidio={\n",
    "        \"PER\": \"PERSON\",\n",
    "        \"LOC\": \"LOCATION\",\n",
    "        \"ORG\": \"ORGANIZATION\",\n",
    "      }\n",
    "\n",
    "      # passes entities from model into parent class\n",
    "      super().__init__(supported_entities=list(self.label2presidio.values()),supported_language=supported_language)\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"No loading is required.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def analyze(\n",
    "        self, text: str, entities: list[str]=None, nlp_artifacts: list[str]=None #NlpArtifacts\n",
    "    ) -> list[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Extracts entities using Transformers pipeline\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # keep max sequence length in mind\n",
    "        predicted_entities = self.pipeline(text)\n",
    "        if len(predicted_entities) >0:\n",
    "          for e in predicted_entities:\n",
    "            converted_entity = self.label2presidio[e[\"entity_group\"]]\n",
    "            if converted_entity in entities or entities is None:\n",
    "              results.append(\n",
    "                  RecognizerResult(\n",
    "                      entity_type=converted_entity,\n",
    "                      start=e[\"start\"],\n",
    "                      end=e[\"end\"],\n",
    "                      score=e[\"score\"]\n",
    "                      )\n",
    "                  )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:30.732321Z",
     "iopub.status.busy": "2024-03-20T02:40:30.731609Z",
     "iopub.status.idle": "2024-03-20T02:40:31.751712Z",
     "shell.execute_reply": "2024-03-20T02:40:31.750364Z",
     "shell.execute_reply.started": "2024-03-20T02:40:30.732289Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:31.754636Z",
     "iopub.status.busy": "2024-03-20T02:40:31.754002Z",
     "iopub.status.idle": "2024-03-20T02:40:31.764593Z",
     "shell.execute_reply": "2024-03-20T02:40:31.763536Z",
     "shell.execute_reply.started": "2024-03-20T02:40:31.754595Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "#%%writefile inference.py\n",
    "\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from typing import List\n",
    "\n",
    "from presidio_analyzer import AnalyzerEngine, EntityRecognizer, RecognizerResult\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts\n",
    "from transformers import pipeline\n",
    "\n",
    "# load spacy model -> workaround\n",
    "import os\n",
    "os.system(\"spacy download en_core_web_lg\")\n",
    "\n",
    "# list of entities: https://microsoft.github.io/presidio/supported_entities/#list-of-supported-entities\n",
    "DEFAULT_ANOYNM_ENTITIES = [\n",
    "    \"CREDIT_CARD\",\n",
    "    \"CRYPTO\",\n",
    "    \"DATE_TIME\",\n",
    "    \"EMAIL_ADDRESS\",\n",
    "    \"IBAN_CODE\",\n",
    "    \"IP_ADDRESS\",\n",
    "    \"NRP\",\n",
    "    \"LOCATION\",\n",
    "    \"PERSON\",\n",
    "    \"PHONE_NUMBER\",\n",
    "    \"MEDICAL_LICENSE\",\n",
    "    \"URL\",\n",
    "    \"ORGANIZATION\"\n",
    "]\n",
    "\n",
    "# init anonymize engine\n",
    "engine = AnonymizerEngine()\n",
    "\n",
    "class HFTransformersRecognizer(EntityRecognizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_id_or_path=None,\n",
    "        aggregation_strategy=\"simple\",\n",
    "        supported_language=\"en\",\n",
    "        ignore_labels=[\"O\", \"MISC\"],\n",
    "    ):\n",
    "        # inits transformers pipeline for given mode or path\n",
    "        self.pipeline = pipeline(\n",
    "            \"token-classification\", model=model_id_or_path, aggregation_strategy=aggregation_strategy, ignore_labels=ignore_labels\n",
    "        )\n",
    "        # map labels to presidio labels\n",
    "        self.label2presidio = {\n",
    "            \"PER\": \"PERSON\",\n",
    "            \"LOC\": \"LOCATION\",\n",
    "            \"ORG\": \"ORGANIZATION\",\n",
    "        }\n",
    "\n",
    "        # passes entities from model into parent class\n",
    "        super().__init__(supported_entities=list(self.label2presidio.values()), supported_language=supported_language)\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"No loading is required.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str] = None, nlp_artifacts: NlpArtifacts = None\n",
    "    ) -> List[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Extracts entities using Transformers pipeline\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        # keep max sequence length in mind\n",
    "        predicted_entities = self.pipeline(text)\n",
    "        if len(predicted_entities) > 0:\n",
    "            for e in predicted_entities:\n",
    "                converted_entity = self.label2presidio[e[\"entity_group\"]]\n",
    "                if converted_entity in entities or entities is None:\n",
    "                    results.append(\n",
    "                        RecognizerResult(\n",
    "                            entity_type=converted_entity, start=e[\"start\"], end=e[\"end\"], score=e[\"score\"]\n",
    "                        )\n",
    "                    )\n",
    "        return results\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    transformers_recognizer = HFTransformersRecognizer(model_dir)\n",
    "    # Set up the engine, loads the NLP module (spaCy model by default) and other PII recognizers\n",
    "    analyzer = AnalyzerEngine()\n",
    "    analyzer.registry.add_recognizer(transformers_recognizer)\n",
    "    return analyzer\n",
    "\n",
    "\n",
    "def predict_fn(data, analyzer):\n",
    "    sentences = data.pop(\"inputs\", data)\n",
    "    if \"parameters\" in data:\n",
    "        anonymization_entities = data[\"parameters\"].get(\"entities\", DEFAULT_ANOYNM_ENTITIES)\n",
    "        anonymize_text = data[\"parameters\"].get(\"anonymize\", False)\n",
    "    else:\n",
    "        anonymization_entities = DEFAULT_ANOYNM_ENTITIES\n",
    "        anonymize_text = False\n",
    "\n",
    "    # identify entities\n",
    "    results = analyzer.analyze(text=sentences, entities=anonymization_entities, language=\"en\")\n",
    "    # anonymize text\n",
    "    if anonymize_text:\n",
    "        result = engine.anonymize(text=sentences, analyzer_results=results)\n",
    "        return {\"anonymized\": result.text}\n",
    "\n",
    "    return {\"found\": [entity.to_dict() for entity in results]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:31.766145Z",
     "iopub.status.busy": "2024-03-20T02:40:31.765843Z",
     "iopub.status.idle": "2024-03-20T02:40:31.778853Z",
     "shell.execute_reply": "2024-03-20T02:40:31.777998Z",
     "shell.execute_reply.started": "2024-03-20T02:40:31.766122Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "\n",
    "presidio-analyzer\n",
    "spacy\n",
    "transformers\n",
    "presidio-anonymizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:31.780147Z",
     "iopub.status.busy": "2024-03-20T02:40:31.779866Z",
     "iopub.status.idle": "2024-03-20T02:40:31.787955Z",
     "shell.execute_reply": "2024-03-20T02:40:31.787160Z",
     "shell.execute_reply.started": "2024-03-20T02:40:31.780125Z"
    }
   },
   "outputs": [],
   "source": [
    "#from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "#huggingface_model = HuggingFaceModel(\n",
    "   #model_data=df_train,       # path to your model and script\n",
    "   #role=role,                    # iam role with permissions to create an Endpoint\n",
    "   #transformers_version=\"4.17\",  # transformers version used\n",
    "   #pytorch_version=\"1.10\",        # pytorch version used\n",
    "   #py_version='py38',            # python version used\n",
    "#)\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "#predictor = huggingface_model.deploy(\n",
    "    #initial_instance_count=1,\n",
    "    #instance_type=\"ml.g4dn.xlarge\"\n",
    "    #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:31.789351Z",
     "iopub.status.busy": "2024-03-20T02:40:31.789087Z",
     "iopub.status.idle": "2024-03-20T02:40:31.800411Z",
     "shell.execute_reply": "2024-03-20T02:40:31.799523Z",
     "shell.execute_reply.started": "2024-03-20T02:40:31.789329Z"
    }
   },
   "outputs": [],
   "source": [
    "payload=\"\"\"\n",
    "Hello, my name is David Johnson and I live in Maine.\n",
    "I work as a software engineer at Amazon.\n",
    "My username for Amazon is djon, my username for ebay is Jnd.\n",
    "You can call me at (123) 456-7890.\n",
    "My credit card number is 4095-2609-9393-4932 and my crypto wallet id is 16Yeky6GMjeNkAiNcBY7ZhrLoMSgg1BoyZ.\n",
    "\n",
    "On September 18 I visited microsoft.com and sent an email to test@presidio.site, from the IP 192.168.0.1.\n",
    "My passport: 191280342 and my phone number: (212) 555-1234.\n",
    "This is a valid International Bank Account Number: IL150120690000003111111. Can you please check the status on bank account 954567876544?\n",
    "Kate's social security number is 078-05-1126.  Her driver license? it is 1234567A.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:31.801962Z",
     "iopub.status.busy": "2024-03-20T02:40:31.801624Z",
     "iopub.status.idle": "2024-03-20T02:40:31.871785Z",
     "shell.execute_reply": "2024-03-20T02:40:31.870824Z",
     "shell.execute_reply.started": "2024-03-20T02:40:31.801934Z"
    }
   },
   "outputs": [],
   "source": [
    "results_english = analyzer.analyze(payload, language='en')\n",
    "print(results_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:31.873902Z",
     "iopub.status.busy": "2024-03-20T02:40:31.873280Z",
     "iopub.status.idle": "2024-03-20T02:40:34.479353Z",
     "shell.execute_reply": "2024-03-20T02:40:34.478377Z",
     "shell.execute_reply.started": "2024-03-20T02:40:31.873870Z"
    }
   },
   "outputs": [],
   "source": [
    "from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "from presidio_analyzer.nlp_engine import NlpEngineProvider\n",
    "\n",
    "LANGUAGES_CONFIG_FILE = \"./docs/analyzer/languages-config.yml\"\n",
    "\n",
    "# Create NLP engine based on configuration file\n",
    "provider = NlpEngineProvider(conf_file=LANGUAGES_CONFIG_FILE)\n",
    "nlp_engine_with_spanish = provider.create_engine()\n",
    "\n",
    "# Pass created NLP engine and supported_languages to the AnalyzerEngine\n",
    "analyzer = AnalyzerEngine(\n",
    "    nlp_engine=nlp_engine_with_spanish, \n",
    "    supported_languages=[\"en\"]#, \"es\"]\n",
    ")\n",
    "\n",
    "# Analyze in different languages\n",
    "#results_spanish = analyzer.analyze(text=\"Mi nombre es David\", language=\"es\")\n",
    "#print(results_spanish)\n",
    "\n",
    "results_english = analyzer.analyze(text=\"My name is David\", language=\"en\")\n",
    "print(results_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:34.480966Z",
     "iopub.status.busy": "2024-03-20T02:40:34.480663Z",
     "iopub.status.idle": "2024-03-20T02:40:34.494616Z",
     "shell.execute_reply": "2024-03-20T02:40:34.493703Z",
     "shell.execute_reply.started": "2024-03-20T02:40:34.480939Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df_test\n",
    "text = df.loc[1, 'full_text']\n",
    "token_list =df.loc[1, 'tokens']\n",
    "whitespace_list = df.loc[1, 'trailing_whitespace']\n",
    "New_text = \"\"   #converts tokens into text and gives locations of each token in the text\n",
    "l = []\n",
    "for i in range(len(token_list)):\n",
    "    if whitespace_list[i] == True:\n",
    "        temp1=len(New_text)\n",
    "        New_text += token_list[i] + ' '\n",
    "        temp2=len(New_text)-1\n",
    "        l.append((token_list[i], temp1, temp2))\n",
    "    if whitespace_list[i] == False:\n",
    "        temp1=len(New_text)\n",
    "        New_text += token_list[i]\n",
    "        temp2=len(New_text)\n",
    "        l.append((token_list[i], temp1, temp2))\n",
    "print(New_text[:100])    \n",
    "print(l[:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:34.495996Z",
     "iopub.status.busy": "2024-03-20T02:40:34.495727Z",
     "iopub.status.idle": "2024-03-20T02:40:34.522636Z",
     "shell.execute_reply": "2024-03-20T02:40:34.521808Z",
     "shell.execute_reply.started": "2024-03-20T02:40:34.495974Z"
    }
   },
   "outputs": [],
   "source": [
    "new = []\n",
    "l_list = []\n",
    "for j in range(len(df_test)):\n",
    "    token_list =df_test.loc[j, 'tokens']\n",
    "    whitespace_list = df_test.loc[j, 'trailing_whitespace']\n",
    "    New_text = \"\"   #converts tokens into text and gives locations of each token in the text\n",
    "    l = []\n",
    "    for i in range(len(token_list)):\n",
    "        if whitespace_list[i] == True:\n",
    "            temp1=len(New_text)\n",
    "            New_text += token_list[i] + ' '\n",
    "            temp2=len(New_text)-1\n",
    "            l.append((token_list[i], temp1, temp2))\n",
    "        if whitespace_list[i] == False:\n",
    "            temp1=len(New_text)\n",
    "            New_text += token_list[i]\n",
    "            temp2=len(New_text)\n",
    "            l.append((token_list[i], temp1, temp2))\n",
    "    new.append(New_text) \n",
    "    l_list.append(l)\n",
    "df_test['text_from_tokens'] = new\n",
    "df_test['tokens_with_locations'] = l_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:34.523959Z",
     "iopub.status.busy": "2024-03-20T02:40:34.523705Z",
     "iopub.status.idle": "2024-03-20T02:40:34.571332Z",
     "shell.execute_reply": "2024-03-20T02:40:34.570373Z",
     "shell.execute_reply.started": "2024-03-20T02:40:34.523936Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:34.573139Z",
     "iopub.status.busy": "2024-03-20T02:40:34.572752Z",
     "iopub.status.idle": "2024-03-20T02:40:34.579207Z",
     "shell.execute_reply": "2024-03-20T02:40:34.578265Z",
     "shell.execute_reply.started": "2024-03-20T02:40:34.573105Z"
    }
   },
   "outputs": [],
   "source": [
    "#summ[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:34.580542Z",
     "iopub.status.busy": "2024-03-20T02:40:34.580306Z",
     "iopub.status.idle": "2024-03-20T02:40:37.031471Z",
     "shell.execute_reply": "2024-03-20T02:40:37.030460Z",
     "shell.execute_reply.started": "2024-03-20T02:40:34.580518Z"
    }
   },
   "outputs": [],
   "source": [
    "summ = {}\n",
    "for i in range(len(df_test)):\n",
    "    payload = df_test.loc[i,'text_from_tokens']\n",
    "    results_english = analyzer.analyze(payload, language='en')\n",
    "    summ[i] = results_english\n",
    "print(summ[0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.032803Z",
     "iopub.status.busy": "2024-03-20T02:40:37.032527Z",
     "iopub.status.idle": "2024-03-20T02:40:37.037726Z",
     "shell.execute_reply": "2024-03-20T02:40:37.036892Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.032780Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(summ), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.038993Z",
     "iopub.status.busy": "2024-03-20T02:40:37.038751Z",
     "iopub.status.idle": "2024-03-20T02:40:37.047062Z",
     "shell.execute_reply": "2024-03-20T02:40:37.046165Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.038972Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_list = {}\n",
    "for i in range(len(summ)):\n",
    "    summary_list[i] = []\n",
    "for j in range(len(summ)):\n",
    "    summary_list[j] = summ[j]\n",
    "    #summary = summ[j]\n",
    "    for i in range(len(summary_list[j])):\n",
    "        recognizer_result = summ[j][i]\n",
    "        summary_list[j][i] = str(recognizer_result)\n",
    "        #if j < 11:\n",
    "            #print(summary_list[j][i], type(summary_list[j][i]))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.048198Z",
     "iopub.status.busy": "2024-03-20T02:40:37.047932Z",
     "iopub.status.idle": "2024-03-20T02:40:37.056984Z",
     "shell.execute_reply": "2024-03-20T02:40:37.056144Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.048177Z"
    }
   },
   "outputs": [],
   "source": [
    "print(summary_list[0])\n",
    "print(summ[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.058557Z",
     "iopub.status.busy": "2024-03-20T02:40:37.058163Z",
     "iopub.status.idle": "2024-03-20T02:40:37.076675Z",
     "shell.execute_reply": "2024-03-20T02:40:37.075849Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.058524Z"
    }
   },
   "outputs": [],
   "source": [
    "#cont = summ[0]\n",
    "summary_dict = {}\n",
    "list_dict ={}\n",
    "for i in range(len(summ)): \n",
    "    summary_dict[i] = []\n",
    "    list_dict[i] = []\n",
    "for j in range(len(summ)):\n",
    "    #summary_dict[j] = [{}]*len(summ[i])\n",
    "    #print(summary_dict[j])\n",
    "    summary = summary_list[j]\n",
    "    for i in range(len(summary)):\n",
    "        summary_dict[j].append({})#[i] = {}\n",
    "    for i in range(len(summary)):\n",
    "        string = summary[i]\n",
    "\n",
    "    #string = cont\n",
    "        start_word = 'type:'\n",
    "        end_word = 'start:'\n",
    "    #end_index = string.find(\"hello\", start_index + len(\"hello\"))\n",
    "        start_index = string.find(start_word)\n",
    "        end_index = string.find(end_word, start_index+len(start_word))\n",
    "        text_between_words = string[start_index+len(start_word):end_index].strip()\n",
    "    #print(text_between_words)\n",
    "    #print(end_index)\n",
    "        start_word1 = 'start:'\n",
    "        end_word1 = 'end:'\n",
    "    #end_index = string.find(\"hello\", start_index + len(\"hello\"))\n",
    "        start_index1 = string.find(start_word1)\n",
    "        end_index1 = string.find(end_word1, start_index1+len(start_word1))\n",
    "        text_between_words1 = string[start_index1+len(start_word1):end_index1].strip()[:-1]\n",
    "        start_word2 = 'end:'\n",
    "        end_word2 = 'score:'\n",
    "    #end_index = string.find(\"hello\", start_index + len(\"hello\"))\n",
    "        start_index2 = string.find(start_word2)\n",
    "        end_index2 = string.find(end_word2, start_index2+len(start_word2))\n",
    "        text_between_words2 = string[start_index2+len(start_word2):end_index2].strip()[:-1]\n",
    "        start_word3 = 'score:'\n",
    "        start_index3 = string.find(start_word3)\n",
    "        end_index3 = start_index3+11 #string.find(end_word3)\n",
    "        text_score = float(string[start_index3 + len(start_word3):end_index3].strip())\n",
    "    #print(start_index3, end_index3, text_score)\n",
    "    #print(text_between_words, text_between_words1, text_between_words2)\n",
    "        if text_between_words == 'PERSON,' and text_score > 0.3:\n",
    "             summary_dict[j][i]['NAME_STUDENT']=(int(text_between_words1),int(text_between_words2))\n",
    "        if text_between_words == 'EMAIL_ADDRESS,' and text_score > 0.3:\n",
    "             summary_dict[j][i]['EMAIL']=(int(text_between_words1),int(text_between_words2))\n",
    "        if text_between_words == 'USERNAME,' and text_score > 0.3:\n",
    "             summary_dict[j][i]['USERNAME,']=(int(text_between_words1),int(text_between_words2))\n",
    "        if (text_between_words == 'US_SSN' or text_between_words == 'US_DRIVER_LICENSE') and text_score > 0.3:\n",
    "             summary_dict[j][i]['ID_NUM,']=(int(text_between_words1), int(text_between_words2))\n",
    "        if text_between_words == 'PHONE_NUMBER,' and text_score > 0.3:\n",
    "             summary_dict[j][i]['PHONE_NUM']=(int(text_between_words1), int(text_between_words2))\n",
    "        if text_between_words == 'URL,' and text_score > 0.3:\n",
    "             summary_dict[j][i]['URL_PERSONAL']=(int(text_between_words1), int(text_between_words2))\n",
    "        if text_between_words == 'LOCATION,' and text_score > 0.3:\n",
    "             summary_dict[j][i]['STREET_ADDRESS']=(int(text_between_words1), int(text_between_words2))\n",
    "    #print(text_score)\n",
    "    #cont = cont[end_index-len(end_word):]\n",
    "    list_dict[j] = []\n",
    "    for i in range(len(summary)):\n",
    "        if len(summary_dict[j][i])>0:\n",
    "        #print(summary_dict[i]) \n",
    "            list_dict[j].append(summary_dict[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.078406Z",
     "iopub.status.busy": "2024-03-20T02:40:37.077846Z",
     "iopub.status.idle": "2024-03-20T02:40:37.088047Z",
     "shell.execute_reply": "2024-03-20T02:40:37.087195Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.078376Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list_dict[0])\n",
    "print(summary_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.089408Z",
     "iopub.status.busy": "2024-03-20T02:40:37.089156Z",
     "iopub.status.idle": "2024-03-20T02:40:37.097612Z",
     "shell.execute_reply": "2024-03-20T02:40:37.096768Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.089387Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(list_tokens[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.099288Z",
     "iopub.status.busy": "2024-03-20T02:40:37.098811Z",
     "iopub.status.idle": "2024-03-20T02:40:37.107349Z",
     "shell.execute_reply": "2024-03-20T02:40:37.106533Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.099258Z"
    }
   },
   "outputs": [],
   "source": [
    "list_dict[0][0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.109088Z",
     "iopub.status.busy": "2024-03-20T02:40:37.108518Z",
     "iopub.status.idle": "2024-03-20T02:40:37.119166Z",
     "shell.execute_reply": "2024-03-20T02:40:37.118369Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.109058Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.loc[0, 'tokens_with_locations'][:11]\n",
    "#df_train.loc[0,'labels']\n",
    "list_tokens = [[]]*len(df_test)\n",
    "for i in range(len(df_test)):\n",
    "    list_tokens[i] = df_test.loc[i, 'tokens_with_locations']\n",
    "#print(list_tokens[11])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.120467Z",
     "iopub.status.busy": "2024-03-20T02:40:37.120224Z",
     "iopub.status.idle": "2024-03-20T02:40:37.128350Z",
     "shell.execute_reply": "2024-03-20T02:40:37.127482Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.120447Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list_dict[0])\n",
    "print(list_tokens[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.129769Z",
     "iopub.status.busy": "2024-03-20T02:40:37.129457Z",
     "iopub.status.idle": "2024-03-20T02:40:37.329469Z",
     "shell.execute_reply": "2024-03-20T02:40:37.328522Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.129738Z"
    }
   },
   "outputs": [],
   "source": [
    "n_token = {}\n",
    "for i in range(len(df_test)):\n",
    "    n_token[i] =[]\n",
    "for l in range(len(df_test)):\n",
    "    for i in range(len(list_tokens[l])):\n",
    "        #for j in range(len(list_dict)):\n",
    "            for k in range(len(list_dict[l])):\n",
    "                j = l\n",
    "                if list_tokens[l][i][1]==list(list_dict[j][k].values())[0][0]:\n",
    "                #print(list_tokens[i], list_dict[j][k])\n",
    "                    if list_tokens[l][i][0] != '\\n' and  list_tokens[l][i][0] != '\\n\\n':\n",
    "                        n_token[l].append((i, 'B-'+list(list_dict[j][k].keys())[0]))\n",
    "                if list_tokens[l][i][2]==list(list_dict[j][k].values())[0][1]:\n",
    "                #print(list_tokens[i], list_dict[j][k])\n",
    "                    if list_tokens[l][i][0] != '\\n' and  list_tokens[l][i][0] != '\\n\\n':\n",
    "                        n_token[l].append((i, 'I-'+list(list_dict[j][k].keys())[0]))\n",
    "                if list_tokens[l][i][1] > list(list_dict[j][k].values())[0][0] and list_tokens[l][i][2]<list(list_dict[j][k].values())[0][1]:\n",
    "                #print(list_tokens[i], list_dict[j][k]) \n",
    "                    if list_tokens[l][i][0] != '\\n' and  list_tokens[l][i][0] != '\\n\\n':\n",
    "                        n_token[l].append((i, 'I-'+list(list_dict[j][k].keys())[0]))\n",
    "print(sorted(list(set(n_token[0]))))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.330923Z",
     "iopub.status.busy": "2024-03-20T02:40:37.330633Z",
     "iopub.status.idle": "2024-03-20T02:40:37.335858Z",
     "shell.execute_reply": "2024-03-20T02:40:37.334976Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.330897Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sorted(list(set(n_token[1])))) \n",
    "print(list_tokens[1][:11], list_tokens[1][464:466])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.337640Z",
     "iopub.status.busy": "2024-03-20T02:40:37.337214Z",
     "iopub.status.idle": "2024-03-20T02:40:37.345632Z",
     "shell.execute_reply": "2024-03-20T02:40:37.344810Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.337597Z"
    }
   },
   "outputs": [],
   "source": [
    "list_n_token ={}\n",
    "token_number_list = {}\n",
    "token_value_list ={}\n",
    "for i in range(len(df_test)):\n",
    "    list_n_token[i] = sorted(list(set(n_token[i])))\n",
    "    token_number_list[i] = []\n",
    "    token_value_list[i] = []\n",
    "    for j in range(len(list_n_token[i])):\n",
    "        token_number_list[i].append(list_n_token[i][j][0])\n",
    "        token_value_list[i].append(list_n_token[i][j][1])\n",
    "#print(list_n_token[0][0])\n",
    "print(token_number_list[1])\n",
    "print(token_value_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.352434Z",
     "iopub.status.busy": "2024-03-20T02:40:37.352173Z",
     "iopub.status.idle": "2024-03-20T02:40:37.413819Z",
     "shell.execute_reply": "2024-03-20T02:40:37.412994Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.352413Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = df_test\n",
    "\n",
    "df_new = df_new.iloc[:, :1]\n",
    "token_info = {}\n",
    "#token_number =[]\n",
    "#token_value = []\n",
    "for i in range(len(df_new)):\n",
    "#for i, row in df_new.iterrows():\n",
    "    token_info[i] = []\n",
    "    for j in range(len(token_number_list[i])):\n",
    "        token_info[i].append((token_number_list[i][j], token_value_list[i][j]))\n",
    "    #token_number.append(token_number_list[i])\n",
    "    #token_value.append(token_value_list[i])\n",
    "#df_new['token_number'] = token_number\n",
    "#df_new['token_value'] = token_value\n",
    "token_infor =[]\n",
    "for i in range(len(df_new)):\n",
    "    token_infor.append(token_info[i])\n",
    "df_new['token_info'] = token_infor\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.415688Z",
     "iopub.status.busy": "2024-03-20T02:40:37.414922Z",
     "iopub.status.idle": "2024-03-20T02:40:37.425900Z",
     "shell.execute_reply": "2024-03-20T02:40:37.424983Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.415663Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = df_new.explode('token_info')\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.427373Z",
     "iopub.status.busy": "2024-03-20T02:40:37.427076Z",
     "iopub.status.idle": "2024-03-20T02:40:37.435419Z",
     "shell.execute_reply": "2024-03-20T02:40:37.434526Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.427340Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = df_new.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.436710Z",
     "iopub.status.busy": "2024-03-20T02:40:37.436429Z",
     "iopub.status.idle": "2024-03-20T02:40:37.454361Z",
     "shell.execute_reply": "2024-03-20T02:40:37.453379Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.436688Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = df_new.reset_index()\n",
    "df_new = df_new.drop('index', axis=1)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.455904Z",
     "iopub.status.busy": "2024-03-20T02:40:37.455547Z",
     "iopub.status.idle": "2024-03-20T02:40:37.464510Z",
     "shell.execute_reply": "2024-03-20T02:40:37.463705Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.455875Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new.loc[0, 'token_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.466267Z",
     "iopub.status.busy": "2024-03-20T02:40:37.465563Z",
     "iopub.status.idle": "2024-03-20T02:40:37.488438Z",
     "shell.execute_reply": "2024-03-20T02:40:37.487518Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.466241Z"
    }
   },
   "outputs": [],
   "source": [
    "token =[]\n",
    "label =[]\n",
    "for i in range(len(df_new)):\n",
    "    token.append(df_new.loc[i, 'token_info'][0])\n",
    "    label.append(df_new.loc[i, 'token_info'][1])\n",
    "df_new['token'] = token\n",
    "df_new['label'] = label\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.489979Z",
     "iopub.status.busy": "2024-03-20T02:40:37.489713Z",
     "iopub.status.idle": "2024-03-20T02:40:37.502620Z",
     "shell.execute_reply": "2024-03-20T02:40:37.501840Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.489957Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new = df_new.drop(['token_info'], axis=1)\n",
    "df_new['row_id'] = df_new.index\n",
    "df_new = df_new.set_index('row_id')\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.504059Z",
     "iopub.status.busy": "2024-03-20T02:40:37.503719Z",
     "iopub.status.idle": "2024-03-20T02:40:37.509944Z",
     "shell.execute_reply": "2024-03-20T02:40:37.509167Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.504030Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def remove_folder_contents(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                remove_folder_contents(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "folder_path = '/kaggle/working'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:41:04.625968Z",
     "iopub.status.busy": "2024-03-20T02:41:04.625058Z",
     "iopub.status.idle": "2024-03-20T02:41:04.631645Z",
     "shell.execute_reply": "2024-03-20T02:41:04.630700Z",
     "shell.execute_reply.started": "2024-03-20T02:41:04.625938Z"
    }
   },
   "outputs": [],
   "source": [
    "df_new.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:41:07.774037Z",
     "iopub.status.busy": "2024-03-20T02:41:07.773649Z",
     "iopub.status.idle": "2024-03-20T02:41:07.789722Z",
     "shell.execute_reply": "2024-03-20T02:41:07.788897Z",
     "shell.execute_reply.started": "2024-03-20T02:41:07.773996Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('submission.csv')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-20T02:40:37.537622Z",
     "iopub.status.busy": "2024-03-20T02:40:37.537259Z",
     "iopub.status.idle": "2024-03-20T02:40:37.553281Z",
     "shell.execute_reply": "2024-03-20T02:40:37.552386Z",
     "shell.execute_reply.started": "2024-03-20T02:40:37.537569Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('/kaggle/input/pii-detection-removal-from-educational-data/sample_submission.csv')\n",
    "df_sample"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 1676899,
     "sourceId": 2749505,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4321892,
     "sourceId": 7427352,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4330323,
     "sourceId": 7440058,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4393949,
     "sourceId": 7545106,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4593097,
     "sourceId": 7835897,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4593132,
     "sourceId": 7835949,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4593155,
     "sourceId": 7835975,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4593164,
     "sourceId": 7835990,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4593181,
     "sourceId": 7836011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4593199,
     "sourceId": 7836033,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4593209,
     "sourceId": 7836044,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4593220,
     "sourceId": 7836071,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
