{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T00:04:07.297739Z",
     "iopub.status.busy": "2024-09-09T00:04:07.296876Z",
     "iopub.status.idle": "2024-09-09T00:04:07.401245Z",
     "shell.execute_reply": "2024-09-09T00:04:07.400151Z",
     "shell.execute_reply.started": "2024-09-09T00:04:07.297697Z"
    }
   },
   "outputs": [],
   "source": [
    "#This notebook is by Anastasia Ruzmaikina for the Kaggle Competition Learning Agency Lab - Automated Essay Scoring 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first automated essay scoring competition to tackle automated grading of student-written essays was twelve years ago. How far have we come from this initial competition? With an updated dataset and light years of new ideas we hope to see if we can get to the latest in automated grading to provide a real impact to overtaxed teachers who continue to have challenges with providing timely feedback, especially in underserved communities.\n",
    "\n",
    "The goal of this competition is to train a model to score student essays. Your efforts are needed to reduce the high expense and time required to hand grade these essays. Reliable automated techniques could allow essays to be introduced in testing, a key indicator of student learning that is currently commonly avoided due to the challenges in grading.\n",
    "\n",
    "Essay writing is an important method to evaluate student learning and performance. It is also time-consuming for educators to grade by hand. Automated Writing Evaluation (AWE) systems can score essays to supplement an educatorâ€™s other efforts. AWEs also allow students to receive regular and timely feedback on their writing. However, due to their costs, many advancements in the field are not widely available to students and educators. Open-source solutions to assess student writing are needed to reach every community with these important educational tools.\n",
    "Previous efforts to develop open-source AWEs have been limited by small datasets that were not nationally diverse or focused on common essay formats. The first Automated Essay Scoring competition scored student-written short-answer responses, however, this is a writing task not often used in the classroom. To improve upon earlier efforts, a more expansive dataset that includes high-quality, realistic classroom writing samples was required. Further, to broaden the impact, the dataset should include samples across economic and location populations to mitigate the potential of algorithmic bias.\n",
    "In this competition, you will work with the largest open-access writing dataset aligned to current standards for student-appropriate assessments. Can you help produce an open-source essay scoring algorithm that improves upon the original Automated Student Assessment Prize (ASAP) competition hosted in 2012?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I use Deberta-V3-Small to score student essays. The accuracy of the scores on the competition test set is 71.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-09T00:04:07.403461Z",
     "iopub.status.busy": "2024-09-09T00:04:07.403093Z",
     "iopub.status.idle": "2024-09-09T00:04:07.427238Z",
     "shell.execute_reply": "2024-09-09T00:04:07.426366Z",
     "shell.execute_reply.started": "2024-09-09T00:04:07.403425Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset,DatasetDict\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from transformers import DebertaV3Model\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "from transformers import TextClassificationPipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments,Trainer\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T00:04:07.428836Z",
     "iopub.status.busy": "2024-09-09T00:04:07.428466Z",
     "iopub.status.idle": "2024-09-09T00:04:08.213908Z",
     "shell.execute_reply": "2024-09-09T00:04:08.212866Z",
     "shell.execute_reply.started": "2024-09-09T00:04:07.428795Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/train.csv')\n",
    "df= df.groupby('score').sample(frac =0.201)\n",
    "#df = df.sample(frac=1)\n",
    "df_train = df.reset_index()\n",
    "df_train['full_text'] = df_train['full_text'].str.lower()\n",
    "df_train['score'] = df_train['score'] - 1\n",
    "df_train['input'] = 'TEXT: '+ df_train.full_text\n",
    "df_train1 = df_train.drop(['index','essay_id','full_text'], axis=1)#\n",
    "ds = Dataset.from_pandas(df_train1)\n",
    "print(ds)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T00:04:08.216387Z",
     "iopub.status.busy": "2024-09-09T00:04:08.216043Z",
     "iopub.status.idle": "2024-09-09T00:04:12.192146Z",
     "shell.execute_reply": "2024-09-09T00:04:12.191235Z",
     "shell.execute_reply.started": "2024-09-09T00:04:08.216322Z"
    }
   },
   "outputs": [],
   "source": [
    "model_nm = '/kaggle/input/debertav3small'#'/kaggle/input/huggingface-deberta-variants/deberta-base/deberta-base'  # '/kaggle/input/huggingface-deberta-variants/deberta-base'# '/kaggle/input/huggingface-deberta-variants/deberta-base-mnli/deberta-base-mnli'  #'/kaggle/input/debertav3small'       #'/kaggle/input/debertav3small'\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "def tok_func(x): return tokz(x[\"input\"])\n",
    "tok_ds = ds.map(tok_func, batched=True)\n",
    "tok_ds = tok_ds.rename_columns({'score':'labels'})\n",
    "dds = tok_ds.train_test_split(0.15, seed=420)\n",
    "print(dds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T00:04:12.193542Z",
     "iopub.status.busy": "2024-09-09T00:04:12.193215Z",
     "iopub.status.idle": "2024-09-09T00:04:12.258288Z",
     "shell.execute_reply": "2024-09-09T00:04:12.257410Z",
     "shell.execute_reply.started": "2024-09-09T00:04:12.193508Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv')\n",
    "df_test['full_text'] = df_test['full_text'].str.lower()\n",
    "df_test['input'] = 'TEXT: '+ df_test.full_text\n",
    "df_test1 = df_test.drop(['full_text','essay_id'], axis=1) #\n",
    "eval_ds = Dataset.from_pandas(df_test1).map(tok_func, batched=True)\n",
    "print(eval_ds)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T00:04:12.259989Z",
     "iopub.status.busy": "2024-09-09T00:04:12.259591Z",
     "iopub.status.idle": "2024-09-09T00:04:12.271920Z",
     "shell.execute_reply": "2024-09-09T00:04:12.270773Z",
     "shell.execute_reply.started": "2024-09-09T00:04:12.259946Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv')\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T00:04:12.273306Z",
     "iopub.status.busy": "2024-09-09T00:04:12.273025Z",
     "iopub.status.idle": "2024-09-09T00:13:53.787142Z",
     "shell.execute_reply": "2024-09-09T00:13:53.785638Z",
     "shell.execute_reply.started": "2024-09-09T00:04:12.273274Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 1\n",
    "epochs = 2\n",
    "lr = 4.15e-6\n",
    "#from transformers import BertForSequenceClassification\n",
    "#model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)\n",
    "\n",
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=6)\n",
    "#model.resize_token_embeddings(len(tokz))\n",
    "#model.compile(optimizer= 'adam' , loss= keras.losses.binary_crossentropy, metrics=['accuracy'])\n",
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                  tokenizer=tokz)#, compute_metrics=compute_metrics\n",
    "trainer.train();\n",
    "\n",
    "preds = trainer.predict(eval_ds).predictions.astype(float)\n",
    "print(preds)\n",
    "preds = np.clip(preds, 0, 1)\n",
    "\n",
    "submission = df_test.essay_id.copy().to_frame()\n",
    "submission[\"score\"] = np.argmax(preds, axis=1)+1\n",
    "\n",
    "#submission[\"generated\"] = submission[\"generated\"].round(1)\n",
    "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-09T00:13:53.788514Z",
     "iopub.status.idle": "2024-09-09T00:13:53.788871Z",
     "shell.execute_reply": "2024-09-09T00:13:53.788716Z",
     "shell.execute_reply.started": "2024-09-09T00:13:53.788698Z"
    }
   },
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('submission.csv')\n",
    "sub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T00:31:42.958284Z",
     "iopub.status.busy": "2024-09-09T00:31:42.957865Z",
     "iopub.status.idle": "2024-09-09T00:31:42.965149Z",
     "shell.execute_reply": "2024-09-09T00:31:42.964238Z",
     "shell.execute_reply.started": "2024-09-09T00:31:42.958244Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def remove_folder_contents(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                remove_folder_contents(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "folder_path = '/kaggle/working'\n",
    "#remove_folder_contents(folder_path)\n",
    "#os.rmdir(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8059942,
     "sourceId": 71485,
     "sourceType": "competition"
    },
    {
     "datasetId": 1369875,
     "sourceId": 3201311,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2105211,
     "sourceId": 3502519,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
