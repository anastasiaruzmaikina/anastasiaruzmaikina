{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":6065,"sourceType":"modelInstanceVersion","modelInstanceId":4686,"modelId":2820},{"sourceId":6068,"sourceType":"modelInstanceVersion","modelInstanceId":4689,"modelId":2821}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#This notebook is by Anastasia Ruzmaikina for Kaggle Competition on Natural Language Processing with Disaster Tweets","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:26:47.184417Z","iopub.execute_input":"2024-09-07T22:26:47.184857Z","iopub.status.idle":"2024-09-07T22:27:13.321832Z","shell.execute_reply.started":"2024-09-07T22:26:47.184818Z","shell.execute_reply":"2024-09-07T22:27:13.320846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it’s not always clear whether a person’s words are actually announcing a disaster. \n\nIn this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. ","metadata":{}},{"cell_type":"markdown","source":"In this notebook (adapted from Kaggle Starter Notebook), I use Deberta-V3 from Keras to classify tweets as disaster tweets and non-disaster tweets. This notebook is 82% accurate on the competition test dataset.","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n!pip install keras-core --upgrade\n!pip install -q keras-nlp --upgrade\n\n# This sample uses Keras Core, the multi-backend version of Keras.\n# The selected backend is TensorFlow (other supported backends are 'jax' and 'torch')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nos.environ['KERAS_BACKEND'] = 'tensorflow'\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)\ndf_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))\nprint(df_train)\nprint(df_test)\ndf_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\ndf_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\ndf_train.drop(['id', 'keyword', 'location'], axis=1, inplace = True)\ndf_test.drop(['id', 'keyword', 'location'], axis=1, inplace = True)\nprint(\"Train Length Stat\")\nprint(df_train[\"length\"].describe())\nprint()\n\nprint(\"Test Length Stat\")\nprint(df_test[\"length\"].describe())\n\nBATCH_SIZE = 32\nNUM_TRAINING_EXAMPLES = df_train.shape[0]\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.2\nSTEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n\nEPOCHS = 6\nAUTO = tf.data.experimental.AUTOTUNE\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:27:13.324005Z","iopub.execute_input":"2024-09-07T22:27:13.324344Z","iopub.status.idle":"2024-09-07T22:27:13.359703Z","shell.execute_reply.started":"2024-09-07T22:27:13.324305Z","shell.execute_reply":"2024-09-07T22:27:13.358748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train[\"text\"].str.lower().str.replace('#', '')\ny = df_train[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n\nX_test = df_test[\"text\"]\n\n# Load a DistilBERT model.\npreset=  \"deberta_v3_base_en\"  #\"distil_bert_base_en_uncased\"\n# Use a shorter sequence length.\npreprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(preset,\n                                                                   sequence_length=160,\n                                                                   name=\"preprocessor_4_tweets\"\n                                                                  )\n#DistilBertPreprocessor\n# Pretrained classifier.\nclassifier = keras_nlp.models.DebertaV3Classifier.from_preset(preset,\n                                                               preprocessor = preprocessor, \n                                                               num_classes=2)#DebertaV3Classifier\nclassifier.summary()\n\n# Compile\n#opt = keras.optimizers.SGD(learning_rate=5e-5) #adam(lr=0.001, decay=1e-6)\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n    #optimizer= opt,  # keras.optimizers.Adam(1e-5),\n    metrics= [\"accuracy\"]  \n)\n# Fit\nhistory = classifier.fit(x=X_train,\n                         y=y_train,\n                         batch_size=BATCH_SIZE,\n                         epochs=EPOCHS, \n                         validation_data=(X_val, y_val)\n                        )\n\ndef displayConfusionMatrix(y_true, y_pred, dataset):\n    disp = ConfusionMatrixDisplay.from_predictions(\n        y_true,\n        np.argmax(y_pred, axis=1),\n        display_labels=[\"Not Disaster\",\"Disaster\"],\n        cmap=plt.cm.Blues\n    )\n\n    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n    f1_score = tp / (tp+((fn+fp)/2))\n\n    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:27:13.361340Z","iopub.execute_input":"2024-09-07T22:27:13.361746Z","iopub.status.idle":"2024-09-07T22:37:12.943804Z","shell.execute_reply.started":"2024-09-07T22:27:13.361703Z","shell.execute_reply":"2024-09-07T22:37:12.943004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = classifier.predict(X_train)\n\ndisplayConfusionMatrix(y_train, y_pred_train, \"Training\")\n\ny_pred_val = classifier.predict(X_val)\n\ndisplayConfusionMatrix(y_val, y_pred_val, \"Validation\")","metadata":{"editable":false,"execution":{"iopub.status.busy":"2024-09-07T22:37:12.945822Z","iopub.execute_input":"2024-09-07T22:37:12.946143Z","iopub.status.idle":"2024-09-07T22:38:18.377783Z","shell.execute_reply.started":"2024-09-07T22:37:12.946110Z","shell.execute_reply":"2024-09-07T22:38:18.376813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nprint(sample_submission.head())\nsample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)\nsample_submission.describe()\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"editable":false,"execution":{"iopub.status.busy":"2024-09-07T22:38:18.379005Z","iopub.execute_input":"2024-09-07T22:38:18.379318Z","iopub.status.idle":"2024-09-07T22:38:42.922064Z","shell.execute_reply.started":"2024-09-07T22:38:18.379284Z","shell.execute_reply":"2024-09-07T22:38:42.921256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('submission.csv')\nsub","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:38:42.923253Z","iopub.execute_input":"2024-09-07T22:38:42.923661Z","iopub.status.idle":"2024-09-07T22:38:42.936287Z","shell.execute_reply.started":"2024-09-07T22:38:42.923616Z","shell.execute_reply":"2024-09-07T22:38:42.935476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working'\n#remove_folder_contents(folder_path)\n#os.rmdir(folder_path)","metadata":{"execution":{"iopub.status.busy":"2024-09-07T22:38:42.937531Z","iopub.execute_input":"2024-09-07T22:38:42.938155Z","iopub.status.idle":"2024-09-07T22:38:42.944684Z","shell.execute_reply.started":"2024-09-07T22:38:42.938112Z","shell.execute_reply":"2024-09-07T22:38:42.943682Z"},"trusted":true},"execution_count":null,"outputs":[]}]}