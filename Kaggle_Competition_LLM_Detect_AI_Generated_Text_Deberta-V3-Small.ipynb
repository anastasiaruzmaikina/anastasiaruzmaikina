{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This notebook is by Anastasia Ruzmaikina for Kaggle Competition LLM - Detect AI Generated Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In recent years, large language models (LLMs) have become increasingly sophisticated, capable of generating text that is difficult to distinguish from human-written text. In this competition, we hope to foster open research and transparency on AI detection techniques applicable in the real world.\n",
    "\n",
    "This competition challenges participants to develop a machine learning model that can accurately detect whether an essay was written by a student or an LLM. The competition dataset comprises a mix of student-written essays and essays generated by a variety of LLMs.\n",
    "\n",
    "Can you help build a model to identify which essay was written by middle and high school students, and which was written using a large language model? With the spread of LLMs, many people fear they will replace or alter work that would usually be done by humans. Educators are especially concerned about their impact on studentsâ€™ skill development, though many remain optimistic that LLMs will ultimately be a useful tool to help students improve their writing skills.\n",
    "\n",
    "At the forefront of academic concerns about LLMs is their potential to enable plagiarism. LLMs are trained on a massive dataset of text and code, which means that they are able to generate text that is very similar to human-written text. For example, students could use LLMs to generate essays that are not their own, missing crucial learning keystones. Your work on this competition can help identify telltale LLM artifacts and advance the state of the art in LLM text detection. By using texts of moderate length on a variety of subjects and multiple, unknown generative models, we aim to replicate typical detection scenarios and incentivize learning features that generalize across models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I use Microsoft Deberta-V3-Small to classify the essays as AI written or human written. The accuracy score for this notebook is 57%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-01-04T17:58:09.334868Z",
     "iopub.status.busy": "2024-01-04T17:58:09.334467Z",
     "iopub.status.idle": "2024-01-04T18:07:20.881255Z",
     "shell.execute_reply": "2024-01-04T18:07:20.880279Z",
     "shell.execute_reply.started": "2024-01-04T17:58:09.334838Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n",
      "KerasNLP version: 0.7.0.dev3\n",
      "/kaggle/input/generated-new/generatednew.csv\n",
      "/kaggle/input/generated2/generated.csv\n",
      "/kaggle/input/llama2-7b-hf/Llama2-7b-hf/config.json\n",
      "/kaggle/input/llama2-7b-hf/Llama2-7b-hf/pytorch_model-00002-of-00002.bin\n",
      "/kaggle/input/llama2-7b-hf/Llama2-7b-hf/tokenizer.json\n",
      "/kaggle/input/llama2-7b-hf/Llama2-7b-hf/tokenizer_config.json\n",
      "/kaggle/input/llama2-7b-hf/Llama2-7b-hf/pytorch_model.bin.index.json\n",
      "/kaggle/input/llama2-7b-hf/Llama2-7b-hf/pytorch_model-00001-of-00002.bin\n",
      "/kaggle/input/llama2-7b-hf/Llama2-7b-hf/special_tokens_map.json\n",
      "/kaggle/input/llama2-7b-hf/Llama2-7b-hf/tokenizer.model\n",
      "/kaggle/input/llama2-7b-hf/Llama2-7b-hf/generation_config.json\n",
      "/kaggle/input/microsoftdeberta-v3-small/spm.model\n",
      "/kaggle/input/microsoftdeberta-v3-small/config.json\n",
      "/kaggle/input/microsoftdeberta-v3-small/README.md\n",
      "/kaggle/input/microsoftdeberta-v3-small/tf_model.h5\n",
      "/kaggle/input/microsoftdeberta-v3-small/tokenizer_config.json\n",
      "/kaggle/input/microsoftdeberta-v3-small/gitattributes\n",
      "/kaggle/input/microsoftdeberta-v3-small/pytorch_model.bin\n",
      "/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\n",
      "/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\n",
      "/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\n",
      "/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\n",
      "/kaggle/input/generated-classes/generatedchat.csv\n",
      "/kaggle/input/generated-classes/generatedsumm.csv\n",
      "            id  prompt_id                                               text  \\\n",
      "0     0059830c          0  Cars. Cars have been around since they became ...   \n",
      "1     005db917          0  Transportation is a large necessity in most co...   \n",
      "2     008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
      "3     00940276          0  How often do you ride in a car? Do you drive a...   \n",
      "4     00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
      "...        ...        ...                                                ...   \n",
      "1373  fe6ff9a5          1  There has been a fuss about the Elector Colleg...   \n",
      "1374  ff669174          0  Limiting car usage has many advantages. Such a...   \n",
      "1375  ffa247e0          0  There's a new trend that has been developing f...   \n",
      "1376  ffc237e9          0  As we all know cars are a big part of our soci...   \n",
      "1377  ffe1ca0d          0  Cars have been around since the 1800's and hav...   \n",
      "\n",
      "      generated  \n",
      "0             0  \n",
      "1             0  \n",
      "2             0  \n",
      "3             0  \n",
      "4             0  \n",
      "...         ...  \n",
      "1373          0  \n",
      "1374          0  \n",
      "1375          0  \n",
      "1376          0  \n",
      "1377          0  \n",
      "\n",
      "[1378 rows x 4 columns]\n",
      "2868\n",
      "3\n",
      "   prompt_id                       prompt_name  \\\n",
      "0          0                   Car-free cities   \n",
      "1          1  Does the electoral college work?   \n",
      "\n",
      "                                        instructions  \\\n",
      "0  Write an explanatory essay to inform fellow ci...   \n",
      "1  Write a letter to your state senator in which ...   \n",
      "\n",
      "                                         source_text  \n",
      "0  # In German Suburb, Life Goes On Without Cars ...  \n",
      "1  # What Is the Electoral College? by the Office...  \n",
      "         id  generated\n",
      "0  0000aaaa        0.1\n",
      "1  1111bbbb        0.9\n",
      "2  2222cccc        0.4\n",
      "Training Set Shape = (2868, 4)\n",
      "Training Set Memory Usage = 0.11 MB\n",
      "Test Set Shape = (3, 3)\n",
      "Test Set Memory Usage = 0.00 MB\n",
      "         id prompt_id                                               text  \\\n",
      "0  0059830c      cars  cars. cars have been around since they became ...   \n",
      "1  005db917      cars  transportation is a large necessity in most co...   \n",
      "2  008f63e3      cars  \"america's love affair with it's vehicles seem...   \n",
      "3  00940276      cars  how often do you ride in a car? do you drive a...   \n",
      "4  00c39458      cars  cars are a wonderful thing. they are perhaps o...   \n",
      "\n",
      "   generated  \n",
      "0          0  \n",
      "1          0  \n",
      "2          0  \n",
      "3          0  \n",
      "4          0  \n",
      "         id prompt_id          text\n",
      "0  0000aaaa       NaN  aaa bbb ccc.\n",
      "1  1111bbbb       NaN  bbb ccc ddd.\n",
      "2  2222cccc       NaN  ccc ddd eee.\n",
      "0       cars. cars have been around since they became ...\n",
      "1       transportation is a large necessity in most co...\n",
      "2       \"america's love affair with it's vehicles seem...\n",
      "3       how often do you ride in a car? do you drive a...\n",
      "4       cars are a wonderful thing. they are perhaps o...\n",
      "                              ...                        \n",
      "1485    car-free day in bogota, colombia, aims to redu...\n",
      "1486     if you could add 30 days to the deadline to v...\n",
      "1487    residents of upscale community in germany have...\n",
      "1488    here's just the first step. (this is a longer ...\n",
      "1489    cars banned in bogota, colombia, to promote al...\n",
      "Name: text, Length: 2868, dtype: object\n",
      "Dataset({\n",
      "    features: ['prompt_id', 'text', 'generated', 'input', '__index_level_0__'],\n",
      "    num_rows: 2868\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857d73f9f25e4a49a65ec976e1b8373e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt_id', 'text', 'labels', 'input', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2437\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt_id', 'text', 'labels', 'input', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 431\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c6fff191624bad897f8a2a4010ed8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/microsoftdeberta-v3-small and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4874' max='4874' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4874/4874 08:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.017202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.91503906  1.14355469]\n",
      " [-0.82226562  1.03125   ]\n",
      " [-0.86914062  1.08496094]]\n"
     ]
    }
   ],
   "source": [
    "#!pip install gdown\n",
    "#import gdown\n",
    "#url = 'https://drive.google.com/uc?id=1-7aCHayx5r2UlYEPHSlH9jjXy1nqgWzA'\n",
    "#output = 'modelnew.h5'\n",
    "#url = 'https://drive.google.com/uc?id=\n",
    "#output =\n",
    "#gdown.download(url,output, quiet=False)\n",
    "! pip install -q datasets\n",
    "\n",
    "#tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras\n",
    "import keras_nlp\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import requests\n",
    "from keras.activations import softmax\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"KerasNLP version:\", keras_nlp.__version__)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "model_nm = '/kaggle/input/microsoftdeberta-v3-small'\n",
    "df_train1 = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\n",
    "#df_train1 = df_train1.loc[df_train1['prompt_id'] == 0]\n",
    "print(df_train1)\n",
    "df_train2 = pd.read_csv(\"/kaggle/input/generated2/generated.csv\")\n",
    "df_train = pd.concat([df_train1, df_train2], axis=0)\n",
    "df_test = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")\n",
    "df_prompt = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\")\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\")\n",
    "from datasets import load_dataset\n",
    "#ds = load_dataset('csv', data_files=['/kaggle/input/llm-detect-ai-generated-text/train_essays.csv', '/kaggle/input/generated2/generated.csv', '/kaggle/input/generated1/generated1.csv'])\n",
    "#print(ds)\n",
    "print(len(df_train))\n",
    "print(len(df_test))\n",
    "print(df_prompt)\n",
    "print(sample_submission)\n",
    "df_train[\"text\"] = df_train[\"text\"].str.lower()\n",
    "#df_train.set_index('id', inplace=True)\n",
    "#df_train['generated'] = df_train['generated'].map({1:'yes', 0:'no'})\n",
    "df_train['prompt_id'] = df_train['prompt_id'].map({1:'electoral', 0:'cars'})\n",
    "df_train[\"text\"] = df_train[\"text\"].str.replace(\"#\", \"\")\n",
    "df_test[\"text\"] = df_test[\"text\"].str.replace(\"#\", \"\" )\n",
    "df_test[\"text\"] = df_test[\"text\"].str.lower()\n",
    "#df_test['generated_text'] = df_test['generated'].map({1:'yes', 0:'no'})\n",
    "df_test['prompt_id'] = df_test['prompt_id'].map({1:'electoral', 0:'cars'})\n",
    "#df_test.set_index('id', inplace=True)\n",
    "print('Training Set Shape = {}'.format(df_train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(df_test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "print(df_train[\"text\"])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset,DatasetDict\n",
    "df_train['input'] = df_train.text #'TEXT: ' + df_train.text + ';  ANC: '+ df_train.prompt_id# + '; ANC2: '+ df_train.id\n",
    "#TEXT2: ' + df_train.generated + ';\n",
    "df_train1 = df_train.drop(['id'], axis=1)\n",
    "ds = Dataset.from_pandas(df_train1)\n",
    "print(ds)\n",
    "\n",
    "#!pip install --upgrade transformers\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from transformers import DebertaV3Model\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "from transformers import TextClassificationPipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments,Trainer\n",
    "#model_nm = '/kaggle/input/debertav3small'\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "def tok_func(x): return tokz(x[\"input\"])\n",
    "tok_ds = ds.map(tok_func, batched=True)\n",
    "tok_ds = tok_ds.rename_columns({'generated':'labels'})\n",
    "dds = tok_ds.train_test_split(0.15, seed=420)\n",
    "print(dds)\n",
    "df_test['input'] = df_test.text #'TEXT: ' + df_test.text + '; ANC: ' + df_test.prompt_id#+ '; ANC2: '+ df_test.id\n",
    "df_test1 = df_test.drop(['id'], axis=1)\n",
    "eval_ds = Dataset.from_pandas(df_test1).map(tok_func, batched=True)\n",
    "bs = 1\n",
    "epochs = 2\n",
    "lr = 4.15e-6\n",
    "#classifier = tf.keras.models.load_model('/kaggle/working/modelnew.h5')\n",
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n",
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "                  tokenizer=tokz)#, compute_metrics=compute_metrics)\n",
    "trainer.train();\n",
    "#pipe = TextClassificationPipeline(model=model, tokenizer=tokz)\n",
    "#prediction = pipe(\"The text to predict\", return_all_scores=True)\n",
    "#print(prediction)\n",
    "preds = trainer.predict(eval_ds).predictions.astype(float)\n",
    "print(preds)\n",
    "preds = np.clip(preds, 0, 1)\n",
    "\n",
    "# Make predictions\n",
    "#predictions = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model (optional)\n",
    "#classifier.evaluate(X_test)\n",
    "submission = df_test.id.copy().to_frame()\n",
    "submission[\"generated\"] = np.argmax(preds, axis=1)#classifier.predict(X_test)\n",
    "#submission[\"generated\"] = submission[\"generated\"].round(1)\n",
    "submission.to_csv(\"/kaggle/working/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-04T18:11:30.449576Z",
     "iopub.status.busy": "2024-01-04T18:11:30.448608Z",
     "iopub.status.idle": "2024-01-04T18:11:32.577517Z",
     "shell.execute_reply": "2024-01-04T18:11:32.576680Z",
     "shell.execute_reply.started": "2024-01-04T18:11:30.449524Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def remove_folder_contents(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                remove_folder_contents(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "folder_path = '/kaggle/working'\n",
    "#remove_folder_contents(folder_path)\n",
    "#os.rmdir(folder_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 6888007,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3601853,
     "sourceId": 6266221,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4200168,
     "sourceId": 7249553,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4213998,
     "sourceId": 7269503,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4234408,
     "sourceId": 7299361,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4244321,
     "sourceId": 7314098,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
